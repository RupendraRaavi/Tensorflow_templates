{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bb41f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddd5d1b",
   "metadata": {},
   "source": [
    "### Reading the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3f1046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = pd.read_csv('POP.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ff1bef",
   "metadata": {},
   "source": [
    "### Converting the file into numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7b357b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_use = my_data['value'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1679be17",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([156309.   , 156527.   , 156731.   , 156943.   , 157140.   ,\n",
       "       157343.   , 157553.   , 157798.   , 158053.   , 158306.   ,\n",
       "       158451.   , 158757.   , 158973.   , 159170.   , 159349.   ,\n",
       "       159556.   , 159745.   , 159956.   , 160184.   , 160449.   ,\n",
       "       160718.   , 160978.   , 161223.   , 161453.   , 161690.   ,\n",
       "       161912.   , 162124.   , 162350.   , 162564.   , 162790.   ,\n",
       "       163026.   , 163290.   , 163570.   , 163847.   , 164107.   ,\n",
       "       164349.   , 164588.   , 164809.   , 165018.   , 165251.   ,\n",
       "       165463.   , 165695.   , 165931.   , 166192.   , 166473.   ,\n",
       "       166755.   , 167023.   , 167270.   , 167513.   , 167746.   ,\n",
       "       167977.   , 168221.   , 168436.   , 168659.   , 168903.   ,\n",
       "       169191.   , 169488.   , 169780.   , 170063.   , 170315.   ,\n",
       "       170571.   , 170806.   , 171029.   , 171271.   , 171501.   ,\n",
       "       171741.   , 171984.   , 172257.   , 172538.   , 172816.   ,\n",
       "       173070.   , 173298.   , 173533.   , 173746.   , 173945.   ,\n",
       "       174176.   , 174397.   , 174639.   , 174882.   , 175143.   ,\n",
       "       175413.   , 175697.   , 175966.   , 176207.   , 176447.   ,\n",
       "       176685.   , 176905.   , 177146.   , 177365.   , 177591.   ,\n",
       "       177830.   , 178101.   , 178376.   , 178657.   , 178921.   ,\n",
       "       179153.   , 179386.   , 179597.   , 179788.   , 180007.   ,\n",
       "       180222.   , 180444.   , 180671.   , 180945.   , 181238.   ,\n",
       "       181528.   , 181796.   , 182042.   , 182287.   , 182520.   ,\n",
       "       182742.   , 182992.   , 183217.   , 183452.   , 183691.   ,\n",
       "       183958.   , 184243.   , 184524.   , 184783.   , 185016.   ,\n",
       "       185242.   , 185452.   , 185650.   , 185874.   , 186087.   ,\n",
       "       186314.   , 186538.   , 186790.   , 187058.   , 187323.   ,\n",
       "       187574.   , 187796.   , 188013.   , 188213.   , 188387.   ,\n",
       "       188580.   , 188790.   , 189018.   , 189242.   , 189496.   ,\n",
       "       189761.   , 190028.   , 190265.   , 190472.   , 190668.   ,\n",
       "       190858.   , 191047.   , 191245.   , 191447.   , 191666.   ,\n",
       "       191889.   , 192131.   , 192376.   , 192631.   , 192847.   ,\n",
       "       193039.   , 193223.   , 193393.   , 193540.   , 193709.   ,\n",
       "       193888.   , 194087.   , 194303.   , 194528.   , 194761.   ,\n",
       "       194997.   , 195195.   , 195372.   , 195539.   , 195688.   ,\n",
       "       195831.   , 195999.   , 196178.   , 196372.   , 196560.   ,\n",
       "       196762.   , 196984.   , 197207.   , 197398.   , 197572.   ,\n",
       "       197736.   , 197892.   , 198037.   , 198206.   , 198363.   ,\n",
       "       198537.   , 198712.   , 198911.   , 199113.   , 199311.   ,\n",
       "       199498.   , 199657.   , 199808.   , 199920.   , 200056.   ,\n",
       "       200208.   , 200361.   , 200536.   , 200706.   , 200898.   ,\n",
       "       201095.   , 201290.   , 201466.   , 201621.   , 201760.   ,\n",
       "       201881.   , 202023.   , 202161.   , 202331.   , 202507.   ,\n",
       "       202677.   , 202877.   , 203090.   , 203302.   , 203500.   ,\n",
       "       203675.   , 203849.   , 204008.   , 204156.   , 204401.   ,\n",
       "       204607.   , 204830.   , 205052.   , 205295.   , 205540.   ,\n",
       "       205788.   , 206024.   , 206238.   , 206466.   , 206668.   ,\n",
       "       206855.   , 207065.   , 207260.   , 207462.   , 207661.   ,\n",
       "       207881.   , 208114.   , 208345.   , 208555.   , 208740.   ,\n",
       "       208917.   , 209061.   , 209212.   , 209386.   , 209545.   ,\n",
       "       209725.   , 209896.   , 210075.   , 210278.   , 210479.   ,\n",
       "       210656.   , 210821.   , 210985.   , 211120.   , 211254.   ,\n",
       "       211420.   , 211577.   , 211746.   , 211909.   , 212092.   ,\n",
       "       212289.   , 212475.   , 212634.   , 212785.   , 212932.   ,\n",
       "       213074.   , 213211.   , 213361.   , 213513.   , 213686.   ,\n",
       "       213854.   , 214042.   , 214246.   , 214451.   , 214625.   ,\n",
       "       214782.   , 214931.   , 215065.   , 215198.   , 215353.   ,\n",
       "       215523.   , 215768.   , 215973.   , 216195.   , 216393.   ,\n",
       "       216587.   , 216771.   , 216931.   , 217095.   , 217249.   ,\n",
       "       217381.   , 217528.   , 217685.   , 217861.   , 218035.   ,\n",
       "       218233.   , 218440.   , 218644.   , 218834.   , 219006.   ,\n",
       "       219179.   , 219344.   , 219504.   , 219684.   , 219859.   ,\n",
       "       220046.   , 220239.   , 220458.   , 220688.   , 220904.   ,\n",
       "       221109.   , 221303.   , 221477.   , 221629.   , 221792.   ,\n",
       "       221991.   , 222176.   , 222379.   , 222585.   , 222805.   ,\n",
       "       223053.   , 223271.   , 223477.   , 223670.   , 223865.   ,\n",
       "       224053.   , 224235.   , 224438.   , 224632.   , 224843.   ,\n",
       "       225055.   , 225295.   , 225547.   , 225801.   , 226027.   ,\n",
       "       226243.   , 226451.   , 226656.   , 226849.   , 227061.   ,\n",
       "       227251.   , 227522.   , 227726.   , 227953.   , 228186.   ,\n",
       "       228417.   , 228612.   , 228779.   , 228937.   , 229071.   ,\n",
       "       229224.   , 229403.   , 229575.   , 229761.   , 229966.   ,\n",
       "       230187.   , 230412.   , 230641.   , 230822.   , 230989.   ,\n",
       "       231157.   , 231313.   , 231470.   , 231645.   , 231809.   ,\n",
       "       231992.   , 232188.   , 232392.   , 232599.   , 232816.   ,\n",
       "       232993.   , 233160.   , 233322.   , 233473.   , 233613.   ,\n",
       "       233781.   , 233922.   , 234118.   , 234307.   , 234501.   ,\n",
       "       234701.   , 234907.   , 235078.   , 235235.   , 235385.   ,\n",
       "       235527.   , 235675.   , 235839.   , 235993.   , 236160.   ,\n",
       "       236348.   , 236549.   , 236760.   , 236976.   , 237159.   ,\n",
       "       237316.   , 237468.   , 237602.   , 237732.   , 237900.   ,\n",
       "       238074.   , 238270.   , 238466.   , 238679.   , 238898.   ,\n",
       "       239113.   , 239307.   , 239477.   , 239638.   , 239788.   ,\n",
       "       239928.   , 240094.   , 240271.   , 240459.   , 240651.   ,\n",
       "       240854.   , 241068.   , 241274.   , 241467.   , 241620.   ,\n",
       "       241784.   , 241930.   , 242079.   , 242252.   , 242423.   ,\n",
       "       242608.   , 242804.   , 243012.   , 243223.   , 243446.   ,\n",
       "       243639.   , 243809.   , 243981.   , 244131.   , 244279.   ,\n",
       "       244445.   , 244610.   , 244806.   , 245021.   , 245240.   ,\n",
       "       245464.   , 245693.   , 245884.   , 246056.   , 246224.   ,\n",
       "       246378.   , 246530.   , 246721.   , 246906.   , 247114.   ,\n",
       "       247342.   , 247573.   , 247816.   , 248067.   , 248281.   ,\n",
       "       248479.   , 248659.   , 248827.   , 249012.   , 249306.   ,\n",
       "       249565.   , 249849.   , 250132.   , 250439.   , 250751.   ,\n",
       "       251057.   , 251346.   , 251626.   , 251889.   , 252135.   ,\n",
       "       252372.   , 252643.   , 252913.   , 253207.   , 253493.   ,\n",
       "       253807.   , 254126.   , 254435.   , 254718.   , 254964.   ,\n",
       "       255214.   , 255448.   , 255703.   , 255992.   , 256285.   ,\n",
       "       256589.   , 256894.   , 257232.   , 257548.   , 257861.   ,\n",
       "       258147.   , 258413.   , 258679.   , 258919.   , 259152.   ,\n",
       "       259414.   , 259680.   , 259963.   , 260255.   , 260566.   ,\n",
       "       260867.   , 261163.   , 261425.   , 261674.   , 261919.   ,\n",
       "       262123.   , 262352.   , 262631.   , 262877.   , 263152.   ,\n",
       "       263436.   , 263724.   , 264017.   , 264301.   , 264559.   ,\n",
       "       264804.   , 265044.   , 265270.   , 265495.   , 265755.   ,\n",
       "       265998.   , 266270.   , 266557.   , 266843.   , 267152.   ,\n",
       "       267456.   , 267715.   , 267943.   , 268151.   , 268364.   ,\n",
       "       268595.   , 268853.   , 269108.   , 269386.   , 269667.   ,\n",
       "       269976.   , 270284.   , 270581.   , 270878.   , 271125.   ,\n",
       "       271360.   , 271585.   , 271821.   , 272083.   , 272342.   ,\n",
       "       272622.   , 272912.   , 273237.   , 273553.   , 273852.   ,\n",
       "       274126.   , 274372.   , 274626.   , 274838.   , 275047.   ,\n",
       "       275304.   , 275564.   , 275836.   , 276115.   , 276418.   ,\n",
       "       276714.   , 277003.   , 277277.   , 277526.   , 277790.   ,\n",
       "       277992.   , 278198.   , 278451.   , 278717.   , 279001.   ,\n",
       "       279295.   , 279602.   , 279903.   , 280203.   , 280471.   ,\n",
       "       280716.   , 280976.   , 281190.   , 281409.   , 281653.   ,\n",
       "       281877.   , 282126.   , 282385.   , 282653.   , 282932.   ,\n",
       "       283201.   , 283453.   , 283696.   , 283920.   , 284137.   ,\n",
       "       284350.   , 284581.   , 284810.   , 285062.   , 285309.   ,\n",
       "       285570.   , 285843.   , 286098.   , 286341.   , 286570.   ,\n",
       "       286788.   , 286994.   , 287190.   , 287397.   , 287623.   ,\n",
       "       287864.   , 288105.   , 288360.   , 288618.   , 288870.   ,\n",
       "       289106.   , 289313.   , 289518.   , 289714.   , 289911.   ,\n",
       "       290125.   , 290346.   , 290584.   , 290820.   , 291072.   ,\n",
       "       291321.   , 291574.   , 291807.   , 292008.   , 292192.   ,\n",
       "       292368.   , 292561.   , 292779.   , 292997.   , 293223.   ,\n",
       "       293463.   , 293719.   , 293971.   , 294230.   , 294466.   ,\n",
       "       294694.   , 294914.   , 295105.   , 295287.   , 295490.   ,\n",
       "       295704.   , 295936.   , 296186.   , 296440.   , 296707.   ,\n",
       "       296972.   , 297207.   , 297431.   , 297647.   , 297854.   ,\n",
       "       298060.   , 298281.   , 298496.   , 298739.   , 298996.   ,\n",
       "       299263.   , 299554.   , 299835.   , 300094.   , 300340.   ,\n",
       "       300574.   , 300802.   , 301021.   , 301254.   , 301483.   ,\n",
       "       301739.   , 302004.   , 302267.   , 302546.   , 302807.   ,\n",
       "       303054.   , 303287.   , 303506.   , 303711.   , 303907.   ,\n",
       "       304117.   , 304323.   , 304556.   , 304798.   , 305045.   ,\n",
       "       305309.   , 305554.   , 305786.   , 306004.   , 306208.   ,\n",
       "       306402.   , 306588.   , 306787.   , 306984.   , 307206.   ,\n",
       "       307439.   , 307685.   , 307946.   , 308189.   , 308418.   ,\n",
       "       308633.   , 308833.   , 309027.   , 309212.   , 309191.211,\n",
       "       309369.053, 309548.502, 309745.698, 309957.775, 310176.466,\n",
       "       310399.958, 310595.764, 310781.705, 310960.74 , 311113.376,\n",
       "       311265.404, 311436.238, 311607.08 , 311791.223, 311997.049,\n",
       "       312205.367, 312429.118, 312644.159, 312829.523, 313009.712,\n",
       "       313183.179, 313338.977, 313499.369, 313667.127, 313830.53 ,\n",
       "       314017.594, 314210.786, 314422.341, 314646.749, 314853.978,\n",
       "       315053.863, 315232.752, 315389.595, 315520.143, 315662.224,\n",
       "       315817.855, 315983.654, 316171.042, 316358.778, 316580.327,\n",
       "       316806.125, 317022.27 , 317228.026, 317411.551, 317593.923,\n",
       "       317753.883, 317917.203, 318089.218, 318269.505, 318464.152,\n",
       "       318662.368, 318893.786, 319125.296, 319353.734, 319564.209,\n",
       "       319746.157, 319928.646, 320074.511, 320230.786, 320402.295,\n",
       "       320583.972, 320773.558, 320978.213, 321202.45 , 321427.611,\n",
       "       321653.019, 321856.265, 322043.071, 322232.862, 322398.098,\n",
       "       322551.468, 322721.212, 322899.995, 323088.466, 323291.042,\n",
       "       323501.361, 323709.851, 323919.726, 324106.527, 324274.912,\n",
       "       324438.195, 324581.497, 324713.97 , 324861.757, 325019.199,\n",
       "       325186.237, 325367.612, 325567.716, 325766.019, 325965.952,\n",
       "       326142.633, 326301.399, 326454.123, 326600.823, 326736.69 ,\n",
       "       326887.866, 327048.704, 327219.14 , 327403.909, 327600.25 ,\n",
       "       327794.788, 327990.95 , 328163.864, 328318.861, 328467.812,\n",
       "       328610.744, 328742.843, 328890.25 , 329047.319, 329213.989,\n",
       "       329394.993, 329591.333, 329785.872, 329982.035, 330154.949,\n",
       "       330309.946])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data_use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05662997",
   "metadata": {},
   "source": [
    "### Creating the window and horizon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df0c7f7",
   "metadata": {},
   "source": [
    "### Hyper-parameters to be considered \n",
    "\n",
    "--> Change the value of 7 in my_target if you are changing the window size ( Window size is the sequence length)\n",
    "\n",
    "--> Modify the window size (sequence_length to see if there is a difference in the model's accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6323ebeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data = my_data_use\n",
    "my_target = my_data_use[7:] \n",
    "Dataset = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "    my_data, my_target, sequence_length=7,sequence_stride= 1, sampling_rate=1, batch_size=len(my_data_use))\n",
    "\n",
    "len(Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a9b44ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "809"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in Dataset:\n",
    "    inputs, targets = batch\n",
    "    break\n",
    "    \n",
    "len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a2f0d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window : [156309. 156527. 156731. 156943. 157140. 157343. 157553.] ----> Label 157798.0\n",
      "window : [156527. 156731. 156943. 157140. 157343. 157553. 157798.] ----> Label 158053.0\n",
      "window : [156731. 156943. 157140. 157343. 157553. 157798. 158053.] ----> Label 158306.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f'window : {inputs[i]} ----> Label {targets[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ec7a569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window : [328742.843 328890.25  329047.319 329213.989 329394.993 329591.333\n",
      " 329785.872] ----> Label 329982.035\n",
      "window : [328890.25  329047.319 329213.989 329394.993 329591.333 329785.872\n",
      " 329982.035] ----> Label 330154.949\n",
      "window : [329047.319 329213.989 329394.993 329591.333 329785.872 329982.035\n",
      " 330154.949] ----> Label 330309.946\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f'window : {inputs[i-3]} ----> Label {targets[i-3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df3d664",
   "metadata": {},
   "source": [
    "### Checking if the values are seperated correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13b1c6bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=bool, numpy=True>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data_use[-1] == targets[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258e3ce3",
   "metadata": {},
   "source": [
    "### Train and split of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da2d9898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_test_splits(windows = inputs , labels = targets, test_split = 0.2):\n",
    "    \n",
    "    split_size = int(len(windows) * (1-test_split))\n",
    "    \n",
    "    train_windows = windows[:split_size]\n",
    "    \n",
    "    train_labels = labels[:split_size]\n",
    "    \n",
    "    test_windows = windows[split_size:]\n",
    "    \n",
    "    test_labels = labels[split_size:]\n",
    "                         \n",
    "    return train_windows ,test_windows, train_labels , test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41b5bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_windows ,test_windows, train_labels , test_labels = make_train_test_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9f1cfaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(647, 162, 647, 162)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_windows) , len(test_windows) , len(train_labels), len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2f6d2f",
   "metadata": {},
   "source": [
    "## Model 1- Simple Dense layers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bb4ad1",
   "metadata": {},
   "source": [
    "### Model check point is initiated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9080d231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "def create_model_checkpoint(model_name , save_path = 'model_experiments'):\n",
    "    return tf.keras.callbacks.ModelCheckpoint(filepath = os.path.join(save_path, model_name),\n",
    "                                             verbosity = 0, # Not print the output,\n",
    "                                             save_best_only = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7c9d31",
   "metadata": {},
   "source": [
    "### Creation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8eddeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = layers.Input(shape = (7,))\n",
    "\n",
    "\n",
    "x = layers.Dense(128, activation = 'relu')(inputs)\n",
    "\n",
    "outputs = layers.Dense(1 , activation = 'linear')(x)\n",
    "\n",
    "model_1 = tf.keras.Model(inputs, outputs , name = 'model_1_dense')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52627bd",
   "metadata": {},
   "source": [
    "### Check model_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba190d9e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 7)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               1024      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,153\n",
      "Trainable params: 1,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3323e8",
   "metadata": {},
   "source": [
    "### Compile and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3d59b09",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/6 [====>.........................] - ETA: 5s - loss: 249970.7656 - mae: 249970.7656 - mse: 64280563712.0000INFO:tensorflow:Assets written to: model_experiments\\model_1_dense\\assets\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 215434.0469 - mae: 215434.0469 - mse: 48336777216.0000 - val_loss: 213814.3281 - val_mae: 213814.3281 - val_mse: 45752786944.0000\n",
      "Epoch 2/100\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 152135.6875 - mae: 152135.6875 - mse: 23901429760.0000INFO:tensorflow:Assets written to: model_experiments\\model_1_dense\\assets\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 122539.5000 - mae: 122539.5000 - mse: 15892895744.0000 - val_loss: 88734.9844 - val_mae: 88734.9844 - val_mse: 7879870464.0000\n",
      "Epoch 3/100\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 64685.8516 - mae: 64685.8516 - mse: 4309417472.0000INFO:tensorflow:Assets written to: model_experiments\\model_1_dense\\assets\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 34715.3828 - mae: 34715.3828 - mse: 1694821376.0000 - val_loss: 27630.8301 - val_mae: 27630.8301 - val_mse: 764250432.0000\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 30430.5840 - mae: 30430.5840 - mse: 997399552.0000 - val_loss: 47392.1406 - val_mae: 47392.1406 - val_mse: 2248128000.0000\n",
      "Epoch 5/100\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 33651.4688 - mae: 33651.4688 - mse: 1167912320.0000INFO:tensorflow:Assets written to: model_experiments\\model_1_dense\\assets\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 24483.8184 - mae: 24483.8184 - mse: 670880000.0000 - val_loss: 1299.9722 - val_mae: 1299.9722 - val_mse: 1698212.5000\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 8507.9277 - mae: 8507.9277 - mse: 91815376.0000 - val_loss: 11642.4189 - val_mae: 11642.4189 - val_mse: 135607088.0000\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 5608.4307 - mae: 5608.4312 - mse: 41549960.0000 - val_loss: 14899.6328 - val_mae: 14899.6328 - val_mse: 222276576.0000\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 6143.3540 - mae: 6143.3540 - mse: 51361556.0000 - val_loss: 6074.8613 - val_mae: 6074.8613 - val_mse: 36914880.0000\n",
      "Epoch 9/100\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4576.2539 - mae: 4576.2539 - mse: 21420264.0000INFO:tensorflow:Assets written to: model_experiments\\model_1_dense\\assets\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 2696.5127 - mae: 2696.5127 - mse: 9746127.0000 - val_loss: 130.5766 - val_mae: 130.5766 - val_mse: 21893.5527\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 538.5943 - mae: 538.5943 - mse: 422492.6875 - val_loss: 1167.7646 - val_mae: 1167.7646 - val_mse: 1385240.2500\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 693.4738 - mae: 693.4738 - mse: 545410.1875 - val_loss: 307.6638 - val_mae: 307.6638 - val_mse: 107090.9766\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 729.0319 - mae: 729.0319 - mse: 868699.8750 - val_loss: 2754.6023 - val_mae: 2754.6023 - val_mse: 7592980.5000\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1443.4094 - mae: 1443.4094 - mse: 2430639.5000 - val_loss: 1907.5439 - val_mae: 1907.5439 - val_mse: 3645284.2500\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1394.3595 - mae: 1394.3595 - mse: 2229777.7500 - val_loss: 1726.0659 - val_mae: 1726.0659 - val_mse: 2986319.2500\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1366.7053 - mae: 1366.7053 - mse: 2169710.2500 - val_loss: 1691.2014 - val_mae: 1691.2014 - val_mse: 2867272.0000\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1434.7479 - mae: 1434.7479 - mse: 2427174.5000 - val_loss: 1164.7444 - val_mae: 1164.7444 - val_mse: 1365395.7500\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1230.6989 - mae: 1230.6989 - mse: 2030504.7500 - val_loss: 2219.1575 - val_mae: 2219.1575 - val_mse: 4930563.5000\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1196.1064 - mae: 1196.1064 - mse: 1694343.3750 - val_loss: 2639.8245 - val_mae: 2639.8245 - val_mse: 6973943.5000\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1207.1056 - mae: 1207.1056 - mse: 1868243.6250 - val_loss: 2943.1218 - val_mae: 2943.1218 - val_mse: 8666958.0000\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1135.7511 - mae: 1135.7511 - mse: 1974063.6250 - val_loss: 1766.8494 - val_mae: 1766.8494 - val_mse: 3128665.2500\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1658.3813 - mae: 1658.3813 - mse: 3630269.2500 - val_loss: 1068.7842 - val_mae: 1068.7842 - val_mse: 1151416.8750\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1071.9904 - mae: 1071.9904 - mse: 1461112.8750 - val_loss: 3410.3064 - val_mae: 3410.3064 - val_mse: 11635050.0000\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2426.0940 - mae: 2426.0940 - mse: 7584826.0000 - val_loss: 3695.3743 - val_mae: 3695.3743 - val_mse: 13701288.0000\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1156.4073 - mae: 1156.4073 - mse: 1909895.8750 - val_loss: 177.5345 - val_mae: 177.5345 - val_mse: 40775.9258\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 619.8975 - mae: 619.8975 - mse: 546965.5000 - val_loss: 427.7062 - val_mae: 427.7062 - val_mse: 199473.2344\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 898.1750 - mae: 898.1750 - mse: 1132562.7500 - val_loss: 3163.8213 - val_mae: 3163.8213 - val_mse: 10049357.0000\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1973.0514 - mae: 1973.0514 - mse: 4912147.0000 - val_loss: 4112.8203 - val_mae: 4112.8203 - val_mse: 16920628.0000\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2181.8689 - mae: 2181.8689 - mse: 5475708.0000 - val_loss: 2814.3691 - val_mae: 2814.3691 - val_mse: 7925766.5000\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2385.5317 - mae: 2385.5317 - mse: 6667896.5000 - val_loss: 2893.1816 - val_mae: 2893.1816 - val_mse: 8407265.0000\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1715.7473 - mae: 1715.7473 - mse: 3796889.2500 - val_loss: 1879.8793 - val_mae: 1879.8793 - val_mse: 3561194.2500\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1011.6661 - mae: 1011.6661 - mse: 1327521.0000 - val_loss: 4182.0601 - val_mae: 4182.0601 - val_mse: 17540928.0000\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2114.6213 - mae: 2114.6213 - mse: 5729375.5000 - val_loss: 473.1424 - val_mae: 473.1424 - val_mse: 235494.2188\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2686.3616 - mae: 2686.3616 - mse: 9218482.0000 - val_loss: 1463.0227 - val_mae: 1463.0227 - val_mse: 2148207.0000\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 846.5947 - mae: 846.5947 - mse: 896772.0625 - val_loss: 201.6742 - val_mae: 201.6742 - val_mse: 53642.5430\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 407.5264 - mae: 407.5264 - mse: 215649.6094 - val_loss: 2410.2942 - val_mae: 2410.2942 - val_mse: 5815097.5000\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1634.8293 - mae: 1634.8293 - mse: 3052181.7500 - val_loss: 1423.5146 - val_mae: 1423.5146 - val_mse: 2034287.3750\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1464.8198 - mae: 1464.8198 - mse: 2423252.7500 - val_loss: 991.4280 - val_mae: 991.4280 - val_mse: 992337.6875\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1292.0011 - mae: 1292.0011 - mse: 1887116.6250 - val_loss: 1465.1499 - val_mae: 1465.1499 - val_mse: 2154427.5000\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1384.4492 - mae: 1384.4492 - mse: 2114567.5000 - val_loss: 1557.3710 - val_mae: 1557.3710 - val_mse: 2432890.2500\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 7ms/step - loss: 1487.1078 - mae: 1487.1078 - mse: 2443056.2500 - val_loss: 498.2735 - val_mae: 498.2735 - val_mse: 259788.2656\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 594.2890 - mae: 594.2890 - mse: 440682.8438 - val_loss: 1883.6708 - val_mae: 1883.6708 - val_mse: 3575492.2500\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 592.0682 - mae: 592.0682 - mse: 578369.8125 - val_loss: 962.9863 - val_mae: 962.9863 - val_mse: 947423.8125\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 369.9393 - mae: 369.9393 - mse: 192679.3750 - val_loss: 610.2427 - val_mae: 610.2427 - val_mse: 390088.6250\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 295.4300 - mae: 295.4300 - mse: 119808.3203 - val_loss: 745.4440 - val_mae: 745.4440 - val_mse: 574270.8750\n",
      "Epoch 45/100\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 300.8910 - mae: 300.8910 - mse: 119511.6641INFO:tensorflow:Assets written to: model_experiments\\model_1_dense\\assets\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 296.8573 - mae: 296.8573 - mse: 119748.5859 - val_loss: 122.8590 - val_mae: 122.8590 - val_mse: 25685.8262\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 747.5508 - mae: 747.5508 - mse: 735863.8125 - val_loss: 1083.7228 - val_mae: 1083.7228 - val_mse: 1195398.7500\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 871.8635 - mae: 871.8635 - mse: 1085890.1250 - val_loss: 1455.5582 - val_mae: 1455.5582 - val_mse: 2126441.5000\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 749.9523 - mae: 749.9523 - mse: 813328.8125 - val_loss: 576.3100 - val_mae: 576.3100 - val_mse: 349602.1875\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 614.5443 - mae: 614.5443 - mse: 562700.4375 - val_loss: 881.3387 - val_mae: 881.3387 - val_mse: 786597.6250\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 702.3825 - mae: 702.3825 - mse: 597722.2500 - val_loss: 1416.0858 - val_mae: 1416.0858 - val_mse: 2028742.3750\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 756.8287 - mae: 756.8287 - mse: 630891.2500 - val_loss: 434.3113 - val_mae: 434.3113 - val_mse: 200434.9688\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 745.0502 - mae: 745.0502 - mse: 606134.0625 - val_loss: 1140.6224 - val_mae: 1140.6224 - val_mse: 1322374.2500\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 661.7789 - mae: 661.7789 - mse: 533136.5625 - val_loss: 493.5561 - val_mae: 493.5561 - val_mse: 255126.5156\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 926.4166 - mae: 926.4166 - mse: 1154675.0000 - val_loss: 195.5827 - val_mae: 195.5827 - val_mse: 49210.7461\n",
      "Epoch 55/100\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 169.3052 - mae: 169.3052 - mse: 45825.7344INFO:tensorflow:Assets written to: model_experiments\\model_1_dense\\assets\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 1384.5197 - mae: 1384.5197 - mse: 2902927.7500 - val_loss: 106.2728 - val_mae: 106.2728 - val_mse: 15579.4443\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1347.9098 - mae: 1347.9098 - mse: 2432742.7500 - val_loss: 926.5773 - val_mae: 926.5773 - val_mse: 868202.5625\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1206.4833 - mae: 1206.4833 - mse: 1729583.2500 - val_loss: 1679.4728 - val_mae: 1679.4728 - val_mse: 2827765.0000\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1161.5248 - mae: 1161.5248 - mse: 1648480.3750 - val_loss: 1366.1659 - val_mae: 1366.1659 - val_mse: 1874481.6250\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1192.3325 - mae: 1192.3325 - mse: 1602908.2500 - val_loss: 1141.5839 - val_mae: 1141.5839 - val_mse: 1312055.8750\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1216.6479 - mae: 1216.6479 - mse: 1663545.6250 - val_loss: 1028.7589 - val_mae: 1028.7589 - val_mse: 1067605.3750\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1275.5979 - mae: 1275.5979 - mse: 1831914.3750 - val_loss: 780.7897 - val_mae: 780.7897 - val_mse: 619885.0000\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1226.8146 - mae: 1226.8146 - mse: 1730638.3750 - val_loss: 1571.4718 - val_mae: 1571.4718 - val_mse: 2476964.2500\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1044.2327 - mae: 1044.2327 - mse: 1415944.7500 - val_loss: 968.8347 - val_mae: 968.8347 - val_mse: 948131.6250\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1305.7924 - mae: 1305.7924 - mse: 2040569.1250 - val_loss: 369.1846 - val_mae: 369.1846 - val_mse: 152467.1562\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 890.0733 - mae: 890.0733 - mse: 1131708.0000 - val_loss: 3293.2607 - val_mae: 3293.2607 - val_mse: 10886535.0000\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2109.7664 - mae: 2109.7664 - mse: 5500286.5000 - val_loss: 4689.7178 - val_mae: 4689.7178 - val_mse: 21999786.0000\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2018.8187 - mae: 2018.8187 - mse: 5050787.5000 - val_loss: 1623.3636 - val_mae: 1623.3636 - val_mse: 2642602.0000\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 791.2926 - mae: 791.2926 - mse: 909828.5625 - val_loss: 255.0575 - val_mae: 255.0575 - val_mse: 77742.4531\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 344.0633 - mae: 344.0633 - mse: 154623.9531 - val_loss: 199.4062 - val_mae: 199.4062 - val_mse: 52734.7734\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 321.3660 - mae: 321.3660 - mse: 157712.6250 - val_loss: 131.5106 - val_mae: 131.5106 - val_mse: 28483.0430\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 849.0575 - mae: 849.0575 - mse: 897354.5000 - val_loss: 2745.9214 - val_mae: 2745.9214 - val_mse: 7575342.0000\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1758.6671 - mae: 1758.6674 - mse: 3678662.2500 - val_loss: 260.9005 - val_mae: 260.9005 - val_mse: 82962.5547\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1379.6864 - mae: 1379.6864 - mse: 2713874.2500 - val_loss: 434.1267 - val_mae: 434.1267 - val_mse: 200271.1406\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1308.6229 - mae: 1308.6229 - mse: 2084504.3750 - val_loss: 697.0025 - val_mae: 697.0025 - val_mse: 496419.5938\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1255.4393 - mae: 1255.4393 - mse: 1836012.2500 - val_loss: 1602.9647 - val_mae: 1602.9647 - val_mse: 2576843.2500\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1007.6354 - mae: 1007.6354 - mse: 1341224.5000 - val_loss: 746.0421 - val_mae: 746.0421 - val_mse: 566974.6875\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1312.7075 - mae: 1312.7075 - mse: 2265020.2500 - val_loss: 576.3999 - val_mae: 576.3999 - val_mse: 349697.0938\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 282.3511 - mae: 282.3511 - mse: 116333.6875 - val_loss: 1610.3179 - val_mae: 1610.3179 - val_mse: 2618101.0000\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 795.7430 - mae: 795.7430 - mse: 832655.9375 - val_loss: 116.2145 - val_mae: 116.2145 - val_mse: 17804.8359\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 329.5849 - mae: 329.5849 - mse: 173805.7500 - val_loss: 992.7342 - val_mae: 992.7342 - val_mse: 994914.7500\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 674.2951 - mae: 674.2951 - mse: 533910.8750 - val_loss: 1868.2134 - val_mae: 1868.2134 - val_mse: 3517344.5000\n",
      "Epoch 82/100\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1086.7874 - mae: 1086.7874 - mse: 1275114.7500INFO:tensorflow:Assets written to: model_experiments\\model_1_dense\\assets\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 795.0309 - mae: 795.0309 - mse: 825094.4375 - val_loss: 96.7425 - val_mae: 96.7425 - val_mse: 16034.2256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 654.8939 - mae: 654.8939 - mse: 694208.1875 - val_loss: 1046.8872 - val_mae: 1046.8872 - val_mse: 1116634.5000\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1011.6508 - mae: 1011.6508 - mse: 1412802.0000 - val_loss: 1773.8141 - val_mae: 1773.8141 - val_mse: 3172740.0000\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1272.3655 - mae: 1272.3655 - mse: 1805816.1250 - val_loss: 1075.0021 - val_mae: 1075.0021 - val_mse: 1176491.7500\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1361.5850 - mae: 1361.5850 - mse: 2525488.0000 - val_loss: 514.2029 - val_mae: 514.2029 - val_mse: 281465.0312\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 750.3391 - mae: 750.3391 - mse: 887064.8125 - val_loss: 99.8927 - val_mae: 99.8927 - val_mse: 17975.0293\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 824.4307 - mae: 824.4307 - mse: 858727.6875 - val_loss: 1727.7870 - val_mae: 1727.7870 - val_mse: 3011184.7500\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1008.7303 - mae: 1008.7303 - mse: 1300419.6250 - val_loss: 4320.2593 - val_mae: 4320.2593 - val_mse: 18717620.0000\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 2379.2314 - mae: 2379.2314 - mse: 7188479.0000 - val_loss: 2175.6545 - val_mae: 2175.6545 - val_mse: 4739451.0000\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1576.2172 - mae: 1576.2172 - mse: 3141375.2500 - val_loss: 4256.0508 - val_mae: 4256.0508 - val_mse: 18119504.0000\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2570.4663 - mae: 2570.4663 - mse: 7808016.0000 - val_loss: 3318.7944 - val_mae: 3318.7944 - val_mse: 11055625.0000\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1210.9711 - mae: 1210.9711 - mse: 1876832.0000 - val_loss: 97.4238 - val_mae: 97.4238 - val_mse: 16539.2832\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 324.3649 - mae: 324.3649 - mse: 181366.7812 - val_loss: 914.8813 - val_mae: 914.8813 - val_mse: 846703.6250\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 698.0179 - mae: 698.0179 - mse: 544921.3750 - val_loss: 1824.4475 - val_mae: 1824.4475 - val_mse: 3355354.5000\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 807.3259 - mae: 807.3259 - mse: 849694.0000 - val_loss: 241.9008 - val_mae: 241.9008 - val_mse: 71261.0312\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 758.3618 - mae: 758.3618 - mse: 659682.7500 - val_loss: 856.9578 - val_mae: 856.9578 - val_mse: 753695.8750\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 845.6724 - mae: 845.6724 - mse: 965098.8125 - val_loss: 1668.9548 - val_mae: 1668.9548 - val_mse: 2792569.2500\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1413.5923 - mae: 1413.5923 - mse: 2543134.7500 - val_loss: 1640.8505 - val_mae: 1640.8505 - val_mse: 2699626.2500\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1283.2231 - mae: 1283.2231 - mse: 2107010.0000 - val_loss: 282.7791 - val_mae: 282.7791 - val_mse: 92500.6016\n"
     ]
    }
   ],
   "source": [
    "model_1.compile(loss='mae',\n",
    "              optimizer='adam',\n",
    "              metrics=['mae','mse'])\n",
    "\n",
    "model_1_history = model_1.fit(x = train_windows,\n",
    "                              y = train_labels,\n",
    "                              epochs = 100,\n",
    "                              verbose = 1,\n",
    "                              batch_size = 128,\n",
    "                             validation_data = (test_windows, test_labels),\n",
    "                             callbacks =[create_model_checkpoint(\n",
    "                                         model_name = 'Dense.h5')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47a5622",
   "metadata": {},
   "source": [
    "### Evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9c62cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 2ms/step - loss: 282.7649 - mae: 282.7649 - mse: 92495.8125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[282.7648620605469, 282.7648620605469, 92495.8125]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.evaluate(test_windows,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece424ab",
   "metadata": {},
   "source": [
    "### Using callback to bring the best model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10957d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = tf.keras.models.load_model('model_experiments/model_1_dense/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b50980",
   "metadata": {},
   "source": [
    "### Helper function's to check the accuracy of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "939b32da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pred(model, input_data):\n",
    "    \n",
    "    forecast = model.predict(input_data)\n",
    "    \n",
    "    return tf.squeeze(forecast\n",
    "                     )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "761402da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import mean_absolute_error, mean_squared_error , mean_absolute_percentage_error\n",
    "\n",
    "def metrics( y_true, y_pred):\n",
    "    \n",
    "    #y_true = tf.cast(y_true, dtype = tf.float32)\n",
    "    #y_pred = tf.cast(y_true, dtype = tf.float32)\n",
    "    \n",
    "    MAE = mean_absolute_error(y_true, y_pred)\n",
    "    \n",
    "    MSE = mean_squared_error(y_true, y_pred)\n",
    "    \n",
    "    RMSE = tf.sqrt(MSE)\n",
    "    \n",
    "    MAPE = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    \n",
    "    #MASE1 = MASE(y_true, y_pred)\n",
    "    \n",
    "    \n",
    "    return {f'MAE : {MAE.numpy()}', \n",
    "            f'MSE : {MSE}',\n",
    "            f'RMSE : {RMSE} ',\n",
    "            f'MAPE : {MAPE} '}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a465cb90",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_1_preds \u001b[38;5;241m=\u001b[39m make_pred(\u001b[43mmodel_1\u001b[49m , test_windows)\n\u001b[0;32m      3\u001b[0m model1_results \u001b[38;5;241m=\u001b[39m metrics(y_true \u001b[38;5;241m=\u001b[39m test_labels , y_pred \u001b[38;5;241m=\u001b[39m model_1_preds)\n\u001b[0;32m      5\u001b[0m model1_results\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_1' is not defined"
     ]
    }
   ],
   "source": [
    "model_1_preds = make_pred(model_1 , test_windows)\n",
    "\n",
    "model1_results = metrics(y_true = test_labels , y_pred = model_1_preds)\n",
    "\n",
    "model1_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ad3250",
   "metadata": {},
   "source": [
    "## Model 2- LSTM use the default activation function by not entering anything and then try tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95655a6c",
   "metadata": {},
   "source": [
    "### Here we need to adjust the size so we added the lambda layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50b33812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "inputs = layers.Input(shape = (7,1))\n",
    "\n",
    "#x = tf.expand_dims(inputs, axis=1)(inputs)\n",
    "\n",
    "x = layers.LSTM(128 , activation = 'relu')(inputs)        #      \n",
    "\n",
    "#x = layers.LSTM(64 , activation = 'relu')(x)\n",
    "\n",
    "x = layers.Dense(32 , activation = 'relu')(x)\n",
    "\n",
    "outputs = layers.Dense(1 , activation = 'linear')(x)\n",
    "\n",
    "model_2 = tf.keras.Model(inputs, outputs , name = 'model_2_LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a049b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2_LSTM\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 7, 1)]            0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 128)               66560     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                4128      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 70,721\n",
      "Trainable params: 70,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "14385b85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 1s 54ms/step - loss: 14815.4775 - val_loss: 16705.0801\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 19130.7363 - val_loss: 2150.3479\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 2091.5984 - val_loss: 3079.8411\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 931.6116 - val_loss: 2057.6167\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 671.9607 - val_loss: 1142.5872\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 489.5757 - val_loss: 617.4373\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 769.9193 - val_loss: 1298.5264\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 708.9448 - val_loss: 588.2400\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 737.4559 - val_loss: 925.2505\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 691.7941 - val_loss: 670.4830\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 1381.5875 - val_loss: 1966.0692\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 1217.5405 - val_loss: 2598.0254\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 1069.2660 - val_loss: 2032.0525\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 1325.6493 - val_loss: 318.7953\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 812.7629 - val_loss: 3007.9971\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 2342.0647 - val_loss: 6679.7808\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 3334.8560 - val_loss: 6304.3506\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 2795.6033 - val_loss: 1012.4462\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 1178.6677 - val_loss: 266.0029\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 359.6683 - val_loss: 291.1429\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 312.3025 - val_loss: 1306.2753\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 466.4639 - val_loss: 1328.7234\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 563.4587 - val_loss: 2145.2942\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 818.8184 - val_loss: 391.9070\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 766.2820 - val_loss: 1182.5359\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 836.2505 - val_loss: 1139.5612\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 1052.8545 - val_loss: 3343.0186\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 1610.4370 - val_loss: 1831.7889\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 1785.5879 - val_loss: 585.8278\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 2202.3584 - val_loss: 4830.0864\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 2412.9050 - val_loss: 1212.6613\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 1723.3665 - val_loss: 5166.7500\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 2423.0920 - val_loss: 388.5438\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 625.0230 - val_loss: 2468.0437\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 1606.5895 - val_loss: 1041.4109\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 1016.4860 - val_loss: 3782.2285\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 2411.8367 - val_loss: 4718.9058\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 1856.3663 - val_loss: 967.0324\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 454.4950 - val_loss: 412.3372\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 364.1297 - val_loss: 319.5021\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 391.8372 - val_loss: 171.2581\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 361.2106 - val_loss: 256.4520\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 413.3942 - val_loss: 857.4456\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 1099.2335 - val_loss: 3625.3911\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 2585.2483 - val_loss: 4219.7822\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 1285.6410 - val_loss: 740.3979\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 925.0900 - val_loss: 1097.7012\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 724.3817 - val_loss: 1067.8003\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 761.8839 - val_loss: 1803.8962\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 827.6625 - val_loss: 225.4591\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 826.9536 - val_loss: 1175.1078\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 433.3866 - val_loss: 1649.6982\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 1204.0266 - val_loss: 127.6418\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 852.3226 - val_loss: 1611.7659\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 833.6555 - val_loss: 674.2232\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 1208.5713 - val_loss: 3501.8782\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 1760.0978 - val_loss: 2501.4248\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 1445.9983 - val_loss: 2716.4519\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 1499.3494 - val_loss: 2890.3406\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 1308.6423 - val_loss: 3455.1821\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 1374.4768 - val_loss: 3693.0767\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 1266.7218 - val_loss: 4444.0000\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 1929.6586 - val_loss: 1768.4940\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 1354.4951 - val_loss: 2631.1956\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 1429.2928 - val_loss: 2745.0913\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 1608.7457 - val_loss: 1574.4319\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 1309.4774 - val_loss: 1813.9568\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 1485.0712 - val_loss: 1240.8347\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 1141.3153 - val_loss: 2853.5493\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 1504.3650 - val_loss: 3433.3491\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 1598.4569 - val_loss: 1125.4518\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 452.9214 - val_loss: 1620.1978\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 1764.5417 - val_loss: 1185.6948\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 2615.8320 - val_loss: 2824.6042\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 993.7883 - val_loss: 530.6730\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 404.7311 - val_loss: 1213.7078\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 501.0624 - val_loss: 1401.9083\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 880.3331 - val_loss: 141.9188\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 23ms/step - loss: 1482.5551 - val_loss: 538.9471\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 352.8414 - val_loss: 324.3137\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 638.8606 - val_loss: 299.7564\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 870.2285 - val_loss: 2666.6208\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 1661.3057 - val_loss: 1172.6144\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 790.7521 - val_loss: 1630.1945\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 851.8128 - val_loss: 271.3522\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 600.0411 - val_loss: 2255.5952\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 909.4651 - val_loss: 353.2492\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 903.4641 - val_loss: 1569.5861\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 823.8767 - val_loss: 130.7527\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 724.6605 - val_loss: 1113.3839\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 727.0312 - val_loss: 734.5457\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 800.5362 - val_loss: 1024.5167\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 826.8841 - val_loss: 322.4117\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 296.8290 - val_loss: 2110.3374\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 866.0204 - val_loss: 884.2490\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 719.5822 - val_loss: 2003.4305\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 708.9241 - val_loss: 1492.8630\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 874.4730 - val_loss: 1639.0232\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 1594.3789 - val_loss: 1202.7510\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 997.0456 - val_loss: 3210.8513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28d288cd460>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile model\n",
    "model_2.compile(loss=\"mae\",\n",
    "                optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# Fit model\n",
    "model_2.fit(train_windows,\n",
    "            train_labels,\n",
    "            batch_size=128, \n",
    "            epochs=100,\n",
    "            verbose=1,\n",
    "            validation_data=(test_windows, test_labels),\n",
    "            callbacks=[create_model_checkpoint(model_name='LSTM.h5')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b42a6829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "model_2 = tf.keras.models.load_model('model_experiments/LSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5984a9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 6ms/step - loss: 127.6418\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "127.64178466796875"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.evaluate(test_windows,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "365d7fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAE : 127.64178466796875',\n",
       " 'MAPE : 0.04038430377840996 ',\n",
       " 'MSE : 22723.728515625',\n",
       " 'RMSE : 150.74391174316406 '}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_2 = tf.keras.models.load_model('model_experiments/model_2_LSTM/')\n",
    "\n",
    "model_2_preds = make_pred(model_2 , test_windows)\n",
    "\n",
    "model2_results = metrics(y_true = test_labels , y_pred = model_2_preds)\n",
    "\n",
    "model2_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e13692c",
   "metadata": {},
   "source": [
    "## Model 3 using the conv1D model - Lambda layers are not accepted so they are removed and 1 is added after 7 (For extra dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "378f44d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "inputs = layers.Input(shape = (7,1)) # 1 is added for extra dimension\n",
    "\n",
    "x = layers.Lambda(lambda y: tf.expand_dims(y, axis=1)) (inputs)\n",
    "\n",
    "x = layers.Conv1D(filters=128, kernel_size=5, padding=\"causal\", activation=\"relu\") (x)       #      \n",
    "\n",
    "#x = layers.LSTM(64 , activation = 'relu')(x)\n",
    "\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "x = layers.Dense(32 , activation = 'relu')(x)\n",
    "\n",
    "outputs = layers.Dense(1 , activation = 'linear')(x)\n",
    "\n",
    "model_3 = tf.keras.Model(inputs, outputs , name = 'model_3_CONV1D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1116f330",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3_CONV1D\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 7)]               0         \n",
      "                                                                 \n",
      " lambda_3 (Lambda)           (None, 1, 7)              0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 1, 128)            4608      \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 128)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 32)                4128      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,769\n",
      "Trainable params: 8,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5df77038",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_experiments\\model_3_CONV1D\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_experiments\\model_3_CONV1D\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_experiments\\model_3_CONV1D\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_experiments\\model_3_CONV1D\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_experiments\\model_3_CONV1D\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_experiments\\model_3_CONV1D\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_experiments\\model_3_CONV1D\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_experiments\\model_3_CONV1D\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_experiments\\model_3_CONV1D\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_experiments\\model_3_CONV1D\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_experiments\\model_3_CONV1D\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_experiments\\model_3_CONV1D\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_experiments\\model_3_CONV1D\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_experiments\\model_3_CONV1D\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_experiments\\model_3_CONV1D\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_experiments\\model_3_CONV1D\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_experiments\\model_3_CONV1D\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_experiments\\model_3_CONV1D\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_experiments\\model_3_CONV1D\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_experiments\\model_3_CONV1D\\assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_experiments\\model_3_CONV1D\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_experiments\\model_3_CONV1D\\assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b27bea1340>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile model\n",
    "model_3.compile(loss=\"mae\",\n",
    "                optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# Fit model\n",
    "model_3.fit(train_windows,\n",
    "            train_labels,\n",
    "            batch_size=128, \n",
    "            epochs=100,\n",
    "            verbose=0,\n",
    "            validation_data=(test_windows, test_labels),\n",
    "            callbacks=[create_model_checkpoint(model_name='Conv1D.h5')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8255ebce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x1b28638b2b0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cddbbc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 2ms/step - loss: 2501.9097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2501.90966796875"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.evaluate(test_windows,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4f258d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAE : 113.64718627929688',\n",
       " 'MAPE : 0.036565784364938736 ',\n",
       " 'MSE : 21349.921875',\n",
       " 'RMSE : 146.11611938476562 '}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3 = tf.keras.models.load_model('model_experiments/model_3_CONV1D/')\n",
    "\n",
    "model_3_preds = make_pred(model_3 , test_windows)\n",
    "\n",
    "model3_results = metrics(y_true = test_labels , y_pred = model_3_preds)\n",
    "\n",
    "model3_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c3eb93",
   "metadata": {},
   "source": [
    "## Multivariate data window and horizon creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "72a0c1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup dataset hyperparameters\n",
    "HORIZON = 1\n",
    "WINDOW_SIZE = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec688de7",
   "metadata": {},
   "source": [
    "### Just create a copy of the file which is already their in our example my_data_use\n",
    "\n",
    "#### In the place of price put the output variable of the exam's question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4250eba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the Bitcoin historical data with block reward feature\n",
    "# Add block_reward column\n",
    "#bitcoin_prices_block = bitcoin_prices.copy()\n",
    "#bitcoin_prices_block[\"block_reward\"] = None\n",
    "\n",
    "bitcoin_prices_windowed = bitcoin_prices_block.copy()     \n",
    "\n",
    "# Add windowed columns\n",
    "for i in range(WINDOW_SIZE): # Shift values for each step in WINDOW_SIZE\n",
    "  bitcoin_prices_windowed[f\"Price+{i+1}\"] = bitcoin_prices_windowed[\"Price\"].shift(periods=i+1)\n",
    "bitcoin_prices_windowed.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbe6148",
   "metadata": {},
   "source": [
    "### Drop the output variable for the x and add only the output variable for the y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf697dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create X & y, remove the NaN's and convert to float32 to prevent TensorFlow errors \n",
    "X = bitcoin_prices_windowed.dropna().drop(\"Price\", axis=1).astype(np.float32) \n",
    "y = bitcoin_prices_windowed.dropna()[\"Price\"].astype(np.float32)\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54606cd9",
   "metadata": {},
   "source": [
    "### Split the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da843f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make train and test sets\n",
    "split_size = int(len(X) * 0.8)\n",
    "X_train, y_train = X[:split_size], y[:split_size]\n",
    "X_test, y_test = X[split_size:], y[split_size:]\n",
    "len(X_train), len(y_train), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b7ce64",
   "metadata": {},
   "source": [
    "## Follow the same process as above to execute the model "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
